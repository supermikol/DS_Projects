## README

### Objective
The goal for this project is to systematically scrape the listings and conduct data analysis in specific Amazon product categories, with the purpose of identifying key metrics that correlate with a higher product category rank. Only 30% of buyers will scroll past the first page, and more often than not, the top 5 listings account for the bulk of the sales. The product's listing order is directly correlated with a product's ranking, and as such, rank is critical in the success of a product.

For the purpose of this project, I set out to scrape 12,000 listings in the duffel bag category, followed by various regression analysis, to determine correlation between various scrapeable features and the rank.

### Linear Regression Models
OLS - 0.425 R^2
OLS with Polynomials - 0.0.421 R^2
ElasticNet - 0.221 R^2 (Not done on log transformed data)
OLS with L2 - 0.416 R^2 (Not done on log transformed data)
OLS with L1 (LassoCV) - 0.0.388 R^2 (Not done on log transformed data)
(Non-linear) Random Forest - 0.678 R^2

### Approach

#### Codebase
The scraping code can be found in `Amazon_Selenium_Scraper.ipynb`.
The data cleaning and analysis can be found in `Amazon_Review_Analysis.ipynb`
I have also built a review analyzer container text analysis, in the notebook titled `Amazon_Review_Analysis.ipynb`.

#### Scraping
Because Amazon's product listings page is dynamically generated, this required utilizing the Selenium package to simulate clicks and scrolling to interact with and extract data from each page, through the use of xpaths.
As the scraper ran, it would export data continuously to a raw csv file.

Some challenges:
- Amazon webpage content is dynamically generated by javascript, so that left me with selenium. Some buttons/links wouldn’t even generate until you actually scrolled down to a certain section of the site (for example, the ‘next page’ button)
- The ID and CLASS tags on each listing varies from one to the next, and sometimes even the layout is completely different, so in the process I had to iterate through 10 listings to build a scraper that would capture most of the possible variance. Even then, there was still a bit of missing data, but thankfully I still had a lot of good data to work with. 

#### Data Cleaning
All the data were extracted as strings. In some cases, entire tables with inconsistent labels were input as a single row. This required a bit of work to identify the correct values associated with each label.

A lot of data was missing. Due to time constraint of this project, I did not experiment with imputation.

Of the 12000 listings, 3983 were missing Volume labels. In a basic comparison between eliminating all the rows with missing volume(less data), vs ignoring the column entirely (more data), it seems that eliminating rows with missing volumn (keeping the feature) results in a higher R^2 value (.425 vs .416). Hence we stuck with keeping the column.

### Regression analysis
Due to skewness of the data, I applied logarithmic transformation to the label data. My original analysis did not apply the transformation, and those results can be seen in the accompanying pdf. The code in the jupyter notebook has been updated to apply this transformation, and my R2 has gone up to 0.42 on validation data and 0.38 on testing data with just simple linear regression. Interestingly, polynomials now have a detrimental impact on the R-score.

Applying random forest boosts the score up to 0.678, which is surprisingly high for the limited data.

### Areas of Improvement
For the data analysis section, I started with all the variables. In retrospect, I realize I should’ve started with fewer features to serve as a baseline model. 
